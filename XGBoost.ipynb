{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Random variables and Tensor Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 'D:\\\\ResearchDataGtx1060\\\\HASOC2020Datasets\\\\'\n",
    "\n",
    "fins_train = ['train'+str(i+1)+'_prepro_hasoc_2020_en_train.csv' for i in range(10)]\n",
    "fins_test = ['eval'+str(i+1)+'_prepro_hasoc_2020_en_train.csv' for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE+fins_train[0])\n",
    "df['task1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task_one(fin):\n",
    "    examples = []\n",
    "    labels = []\n",
    "    df = pd.read_csv(fin)\n",
    "    for idx in df.index:\n",
    "        examples.append(df.loc[idx, 'text'])\n",
    "        if df.loc[idx, 'task1']=='NOT':\n",
    "            labels.append(0)\n",
    "        elif df.loc[idx, 'task1']=='HOF':\n",
    "            labels.append(1)\n",
    "    return np.array(examples), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_task_one(BASE+fins_train[0])\n",
    "X_test, y_test = load_task_one(BASE+fins_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "Xtrain = vectorizer.fit_transform(X_train)\n",
    "Xtrain = Xtrain.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = vectorizer.transform(X_test)\n",
    "Xtest = Xtest.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyper parameters to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "                 'n_estimators': [130],\n",
    "                 'max_depth': [140]\n",
    "             }# best value for english\n",
    "\n",
    "# param_grid = {\n",
    "#                  'n_estimators': [120],\n",
    "#                  'max_depth': [130]\n",
    "#              }# best value for german\n",
    "\n",
    "# param_grid = {\n",
    "#                  'n_estimators': [120],\n",
    "#                  'max_depth': [130]\n",
    "#              }# best value for hindi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model no training data\n",
    "clf_XGBClassifier = XGBClassifier()\n",
    "#grid_xgbc = GridSearchCV(clf_XGBClassifier, param_grid, cv=1)\n",
    "grid_xgbc = XGBClassifier(n_estimators=125, max_depth=140, n_jobs=8)\n",
    "grid_xgbc.fit(Xtrain, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "#print(grid_xgbc.best_params_)\n",
    "print()\n",
    "\n",
    "print(grid_xgbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predicted = y_test, grid_xgbc.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted = np.array(predicted)\n",
    "\n",
    "tp = np.count_nonzero(predicted * actual)\n",
    "tn = np.count_nonzero((predicted - 1) * (actual - 1))\n",
    "fp = np.count_nonzero(predicted * (actual - 1))\n",
    "fn = np.count_nonzero((predicted - 1) * actual)\n",
    "\n",
    "print('True Positive\\t' + str(tp))\n",
    "print('True Negative\\t' + str(tn))\n",
    "print('False Positive\\t' + str(fp))\n",
    "print('False Negative\\t' + str(fn))\n",
    "\n",
    "accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "cohen_kappa_score = cohen_kappa_score(predicted, actual)\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(actual, predicted)\n",
    "auc_val = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc_val = roc_auc_score(actual, predicted)\n",
    "\n",
    "print('Accuracy\\t' + str(accuracy))\n",
    "print('Precision\\t' + str(precision))\n",
    "print('Recall\\t' + str(recall))\n",
    "print('f-measure\\t' + str(fmeasure))\n",
    "print('cohen_kappa_score\\t' + str(cohen_kappa_score))\n",
    "print('auc\\t' + str(auc_val))\n",
    "print('roc_auc\\t' + str(roc_auc_val))\n",
    "\n",
    "#print(\"Average of ROC-AUC score: %.3f\" % roc_auc_score(ytest, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'XGBoost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "out_string = '=========='+str(now)+'==============\\n'\n",
    "out_string += 'Language:\\t'+'\\n'\n",
    "out_string += 'Dataset:\\t' + '\\n'\n",
    "out_string += 'Task:\\t' + '\\n'\n",
    "out_string += str('Model Name:\\t' + model_name+'\\n')\n",
    "out_string += '-------------------------------------------------' + '\\n'\n",
    "\n",
    "out_string += 'Total Samples:\\t' + str(len(actual)) + '\\n'\n",
    "out_string += 'Positive Samples:\\t' + str(sum(actual)) + '\\n'\n",
    "out_string += 'Negative Samples:\\t' + str(len(actual)-sum(actual)) + '\\n'\n",
    "\n",
    "out_string += 'True Positive:\\t' + str(tp) + '\\n'\n",
    "out_string += 'True Negative:\\t' + str(tn) + '\\n'\n",
    "out_string += 'False Positive:\\t' + str(fp) + '\\n'\n",
    "out_string += 'False Negative:\\t' + str(fn) + '\\n'\n",
    "\n",
    "out_string += 'Accuracy:\\t' + str(accuracy) + '\\n'\n",
    "out_string += 'Precision:\\t' + str(precision) + '\\n'\n",
    "out_string += 'Recall:\\t' + str(recall) + '\\n'\n",
    "out_string += 'F-measure:\\t' + str(fmeasure) + '\\n'\n",
    "out_string += 'Cohen_Kappa_Score:\\t' + str(cohen_kappa_score) + '\\n'\n",
    "out_string += 'AUC:\\t' + str(auc_val) + '\\n'\n",
    "out_string += 'ROC_AUC:\\t' + str(roc_auc_val) + '\\n'\n",
    "out_string += '\\n'\n",
    "out_string += classification_report(actual, predicted)\n",
    "out_string += '\\n'\n",
    "print(out_string)\n",
    "with open('BaselineResults_eng_task1.txt', 'a+') as FO:\n",
    "    FO.write(out_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
